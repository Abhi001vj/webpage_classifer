{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabhilash001vj\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We are building an image classifer that can classify website screeshots into the following categories.\n",
    "- Marketplace\n",
    "- Forum\n",
    "- General (neither market nor forum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"marketplace\",\"forum\",\"general\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_process_and_train.ipynb', 'webs-te-image-classifier.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "folder_0_images = glob.glob(\"../data/circl-ail-dataset-01/folder_0/*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_1_images = glob.glob(\"../data/circl-ail-dataset-01/folder_1_SECONDPASS/*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 4000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folder_1_images), len(folder_0_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "label_file = \"../data/circl-ail-dataset-01/labels_0.json\"\n",
    "with open(label_file) as f:\n",
    "    folder_0_labels = json.load(f)\n",
    "label_file = \"../data/circl-ail-dataset-01/labels_1.json\"\n",
    "with open(label_file) as f:\n",
    "    folder_1_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "data_dir = \"../data/website_images_1000/\"\n",
    "common_class = \"general\"\n",
    "label_count = {}\n",
    "max_count = 1000\n",
    "os.makedirs(os.path.join(data_dir,common_class), exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "for img_path in folder_0_images:\n",
    "    filename  = os.path.basename(img_path)\n",
    "    label_list = folder_0_labels[filename]\n",
    "    found_label = False\n",
    "    for label in labels:\n",
    "        os.makedirs(os.path.join(data_dir,label), exist_ok=True)\n",
    "        if label in \" \".join(label_list) and label_count.get(label, 0) < max_count:\n",
    "            shutil.copy(img_path, os.path.join(data_dir,label,filename))\n",
    "            found_label = True\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "            break\n",
    "    if not found_label and label_count.get(common_class, 0) < max_count:\n",
    "        label_count[common_class] = label_count.get(common_class, 0 )+ 1\n",
    "        shutil.copy(img_path, os.path.join(data_dir,common_class,filename))\n",
    "\n",
    "\n",
    "os.makedirs(os.path.join(data_dir,common_class), exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "for img_path in folder_1_images:\n",
    "    filename  = os.path.basename(img_path)\n",
    "    label_list = folder_1_labels[filename]\n",
    "    found_label = False\n",
    "    for label in labels:\n",
    "        os.makedirs(os.path.join(data_dir,label), exist_ok=True)\n",
    "        if label in \" \".join(label_list) and label_count.get(label, 0) < max_count:\n",
    "            shutil.copy(img_path, os.path.join(data_dir,label,filename))\n",
    "            found_label = True\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "            break\n",
    "    if not found_label and label_count.get(common_class, 0) < max_count:\n",
    "        label_count[common_class] = label_count.get(common_class, 0) + 1\n",
    "        shutil.copy(img_path, os.path.join(data_dir,common_class,filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dark-web:topic=\"drugs-narcotics\"'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Label  Image Count\n",
      "0        forum         1000\n",
      "1      general         1000\n",
      "2  marketplace         1000\n",
      "3        total         3000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_image_count(root_dir):\n",
    "    label_dirs = [dir for dir in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, dir))]\n",
    "\n",
    "    count_dict = {}\n",
    "    total_count = 0\n",
    "\n",
    "    # Iterate through each directory (label)\n",
    "    for dir in label_dirs:\n",
    "        dir_path = os.path.join(root_dir, dir)\n",
    "\n",
    "        # Count only image files, assuming they have extensions .jpg, .png, .jpeg, .tiff, .bmp, .gif\n",
    "        image_files = [f for f in os.listdir(dir_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))]\n",
    "        image_count = len(image_files)\n",
    "\n",
    "        count_dict[dir] = image_count\n",
    "        total_count += image_count\n",
    "\n",
    "    count_dict['total'] = total_count\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "root_dir = \"../data/website_images_1000/\"  # replace with your data directory\n",
    "counts = get_image_count(root_dir)\n",
    "# Create a dataframe from counts dictionary and print\n",
    "df = pd.DataFrame(list(counts.items()), columns=['Label', 'Image Count'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wandb.ai/mostafaibrahim17/ml-articles/reports/A-Gentle-Introduction-to-Image-Classification--VmlldzozNDI4MjQ4\n",
    "# https://wandb.ai/stacey/mendeleev/reports/Tables-Tutorial-Visualize-Data-for-Image-Classification--VmlldzozNjE3NjA\n",
    "# https://colab.research.google.com/github/wandb/examples/blob/master/colabs/datasets-predictions/Image_Classification_with_Tables.ipynb#scrollTo=ios7mrU2BcIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "DATA_SRC = \"../data/website_images/\"\n",
    "# source directory for all raw data\n",
    "SRC = DATA_SRC\n",
    "PREFIX = \"website_screeshots\" # convenient for tracking local data\n",
    "PROJECT_NAME = \"Website Classification\"\n",
    "IMAGES_PER_LABEL = 0\n",
    "# number of images per class label\n",
    "# the total number of images is 10X this (10 classes)\n",
    "TOTAL_IMAGES = 3000 # IMAGES_PER_LABEL * 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Upload raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\kaggle2022\\web_page_classifier\\notebooks\\wandb\\run-20230521_175230-hj2qd9rs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/abhilash001vj/Website%20Classification/runs/hj2qd9rs\" target=\"_blank\">confused-field-2</a></strong> to <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification\" target=\"_blank\">https://wandb.ai/abhilash001vj/Website%20Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification/runs/hj2qd9rs\" target=\"_blank\">https://wandb.ai/abhilash001vj/Website%20Classification/runs/hj2qd9rs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-field-2</strong> at: <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification/runs/hj2qd9rs\" target=\"_blank\">https://wandb.ai/abhilash001vj/Website%20Classification/runs/hj2qd9rs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230521_175230-hj2qd9rs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if this is a substantially new dataset, give it a new name\n",
    "# this will create a whole new placeholder (Artifact) for this dataset\n",
    "# instead of just incrementing a version of the old dataset\n",
    "RAW_DATA_AT = \"_\".join([PREFIX, \"raw_data\", str(TOTAL_IMAGES)])\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"upload\")\n",
    "# create an artifact for all the raw data\n",
    "raw_data_at = wandb.Artifact(RAW_DATA_AT, type=\"raw_data\")\n",
    "\n",
    "# SRC_DIR contains 10 folders, one for each of 10 class labels\n",
    "# each folder contains images of the corresponding class\n",
    "labels = os.listdir(SRC)\n",
    "for l in labels:\n",
    "  imgs_per_label = os.path.join(SRC, l)\n",
    "  if os.path.isdir(imgs_per_label):\n",
    "    # filter out \"DS_Store\"\n",
    "    imgs = [i for i in os.listdir(imgs_per_label) if not i.startswith(\".DS\")]\n",
    "    # randomize the order\n",
    "    shuffle(imgs)\n",
    "    img_file_ids = imgs[:IMAGES_PER_LABEL]\n",
    "    for f in img_file_ids:\n",
    "      file_path = os.path.join(SRC, l, f)\n",
    "      # add file to artifact by full path\n",
    "      raw_data_at.add_file(file_path, name=l + \"/\" + f)\n",
    "\n",
    "# save artifact to W&B\n",
    "run.log_artifact(raw_data_at)\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Split raw data to prepare for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 2400.0, 'val': 300.0, 'test': 300.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"train\" : ( TOTAL_IMAGES * 0.8 ),  \"val\" : ( TOTAL_IMAGES * 0.1 ), \"test\": ( TOTAL_IMAGES * 0.1 ) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\kaggle2022\\web_page_classifier\\notebooks\\wandb\\run-20230521_180902-13jfboot</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/abhilash001vj/Website%20Classification/runs/13jfboot\" target=\"_blank\">worthy-shape-3</a></strong> to <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification\" target=\"_blank\">https://wandb.ai/abhilash001vj/Website%20Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification/runs/13jfboot\" target=\"_blank\">https://wandb.ai/abhilash001vj/Website%20Classification/runs/13jfboot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '.\\\\artifacts\\\\website_screeshots_raw_data_3000-v0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# create a table with columns we want to track/compare\u001b[39;00m\n\u001b[0;32m     24\u001b[0m preview_dt \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mTable(columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m---> 26\u001b[0m labels \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(data_dir)\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m labels:\n\u001b[0;32m     28\u001b[0m   \u001b[39mif\u001b[39;00m l\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m): \u001b[39m# skip non-label file\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '.\\\\artifacts\\\\website_screeshots_raw_data_3000-v0'"
     ]
    }
   ],
   "source": [
    "BALANCED_SPLITS = {\"train\" : ( TOTAL_IMAGES * 0.8 ),  \"val\" : ( TOTAL_IMAGES * 0.1 ), \"test\": ( TOTAL_IMAGES * 0.1 ) }\n",
    "# if this is a substantially different dataset, give it a new name\n",
    "# this will create a whole new placeholder (Artifact) for this split\n",
    "# instead of just incrementing a version of the old data split\n",
    "SPLIT_DATA_AT = \"_\".join([PREFIX, \"80-10-10\", str(TOTAL_IMAGES)])\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"data_split\")\n",
    "\n",
    "# create balanced train, val, test splits\n",
    "# each count is the number of images per label\n",
    "SPLIT_COUNTS = BALANCED_SPLITS\n",
    "\n",
    "# find the most recent (\"latest\") version of the full raw data\n",
    "# you can of course pass around programmatic aliases and not string literals\n",
    "# note: RAW_DATA_AT is defined in the previous cell—if you're running\n",
    "# just this step, you may need to hardcode it\n",
    "data_at = run.use_artifact(RAW_DATA_AT + \":latest\")\n",
    "# download it locally (for illustration purposes/across hardware; you can\n",
    "# also sync/version artifacts by reference)\n",
    "data_dir = data_at.download()\n",
    "\n",
    "data_split_at = wandb.Artifact(SPLIT_DATA_AT, type=\"balanced_data\")\n",
    "\n",
    "# create a table with columns we want to track/compare\n",
    "preview_dt = wandb.Table(columns=[\"id\", \"image\", \"label\", \"split\"])\n",
    "\n",
    "labels = os.listdir(data_dir)\n",
    "for l in labels:\n",
    "  if l.startswith(\".\"): # skip non-label file\n",
    "    continue\n",
    "  imgs_per_label = os.listdir(os.path.join(data_dir, l))\n",
    "  shuffle(imgs_per_label)\n",
    "  start_id = 0\n",
    "  for split, count in SPLIT_COUNTS.items():\n",
    "    # take a subset\n",
    "    split_imgs = imgs_per_label[start_id:start_id+count]\n",
    "    for img_file in split_imgs:\n",
    "      f_id = img_file.split(\".\")[0]\n",
    "      full_path = os.path.join(data_dir, l, img_file)\n",
    "      # add file to artifact by full path\n",
    "      # note: pass the label to the name parameter to retain it in\n",
    "      # the data structure \n",
    "      data_split_at.add_file(full_path, name = os.path.join(split, l, img_file))\n",
    "      # add a preview of the image\n",
    "      # if SIZE == \"LARGE\": # skip for the largest dataset for efficiency\n",
    "      #   continue\n",
    "      if split != \"test\":\n",
    "        preview_dt.add_data(f_id, wandb.Image(full_path), l, split)\n",
    "      else:\n",
    "        # pretend we have unlabeled test data\n",
    "        # (replace \"unknown\" with l if you'd like to keep the labels :)\n",
    "        preview_dt.add_data(f_id, wandb.Image(full_path), \"unknown\", split)\n",
    "    start_id += count\n",
    "\n",
    "# log artifact to W&B\n",
    "data_split_at.add(preview_dt, \"data_split\")\n",
    "run.log_artifact(data_split_at)\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train with artifacts and save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT CONFIG\n",
    "#------------------------\n",
    "# Core globals to modify\n",
    "NUM_EPOCHS = 5 # set low for demo purposes, try 3, or 5, or as many as you like\n",
    "\n",
    "\n",
    "# optional globals to modify\n",
    "# set to a custom name to help keep your experiments organized\n",
    "RUN_NAME = \"base_model\" \n",
    "# change this if you'd like start a new set of comparable Tables\n",
    "# (only Tables logged to the same key can be compared)\n",
    "VAL_TABLE_NAME = \"predictions\" \n",
    "\n",
    "# hyperparams set low for demo/training speed\n",
    "# if you set these higher, be mindful of how many items are in\n",
    "# the dataset artifacts you chose by setting the SIZE at the top\n",
    "NUM_TRAIN = BALANCED_SPLITS[\"train\"]\n",
    "NUM_VAL = BALANCED_SPLITS[\"val\"]\n",
    "\n",
    "# enforced max for this is ceil(NUM_VAL/batch_size)\n",
    "NUM_LOG_BATCHES = 16\n",
    "\n",
    "# ARTIFACTS CONFIG\n",
    "#------------------------\n",
    "# training data artifact to load\n",
    "TRAIN_DATA_AT = PREFIX + \"_80-10-10_\" + str(TOTAL_IMAGES)\n",
    "\n",
    "# model name\n",
    "# if you want to train a sufficiently different model, give this a new name\n",
    "# to start a new lineage for the model, instead of just incrementing the\n",
    "# version of the old model\n",
    "MODEL_NAME = \"iv3_finetuned\"\n",
    "\n",
    "# folder in which to save the final, trained model\n",
    "# if you want to train a sufficiently different model, give this a new name\n",
    "# to start a new lineage for the model, instead of just incrementing the\n",
    "# version of the old model\n",
    "SAVE_MODEL_DIR = \"finetune_iv3_keras\"\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras.applications.resnet import InceptionV3\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# experiment configuration saved to W&B\n",
    "CFG = {\n",
    "  \"num_train\" : NUM_TRAIN,\n",
    "  \"num_val\" : NUM_VAL,\n",
    "  \"num_classes\" : 10,\n",
    "  \"fc_size\" : 1024,\n",
    "  \"epochs\" : NUM_EPOCHS,\n",
    "  \"batch_size\" : 32,\n",
    "\n",
    "  # inceptionV3 settings\n",
    "  \"img_width\" : 299,\n",
    "  \"img_height\": 299\n",
    "}\n",
    "\n",
    "# number of validation data batches to log/use when computing metrics\n",
    "# at the end of each epoch\n",
    "max_log_batches = int(np.ceil(float(CFG[\"num_val\"])/float(CFG[\"batch_size\"])))\n",
    "# change this min to max to log ALL the available images to a Table\n",
    "CFG[\"num_log_batches\"] = min(max_log_batches, NUM_LOG_BATCHES)\n",
    "\n",
    "def finetune_inception_model(fc_size, num_classes):\n",
    "  \"\"\"Load InceptionV3 with ImageNet weights, freeze it,\n",
    "  and attach a finetuning top for this classification task\"\"\"\n",
    "  # load InceptionV3 as base\n",
    "  base = InceptionV3(weights=\"imagenet\", include_top=\"False\")\n",
    "  # freeze base layers\n",
    "  for layer in base.layers:\n",
    "    layer.trainable = False\n",
    "  x = base.get_layer('mixed10').output \n",
    "\n",
    "  # attach a fine-tuning layer\n",
    "  x = GlobalAveragePooling2D()(x)\n",
    "  x = Dense(fc_size, activation='relu')(x)\n",
    "  guesses = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "  model = Model(inputs=base.input, outputs=guesses)\n",
    "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def train():\n",
    "  \"\"\" Main training loop which freezes the InceptionV3 layers of the model\n",
    "  and only trains the new top layers on the new data. A subsequent training\n",
    "  phase might unfreeze all the layers and finetune the whole model on the new data\"\"\" \n",
    "  run = wandb.init(project=PROJECT_NAME, name=RUN_NAME, job_type=\"train\", config=CFG)\n",
    "  cfg = wandb.config\n",
    "\n",
    "  # locate and download training and validation data\n",
    "  data_at = TRAIN_DATA_AT + \":latest\"\n",
    "  data = run.use_artifact(data_at, type=\"balanced_data\")\n",
    "  data_dir = data.download()\n",
    "  train_dir = os.path.join(data_dir, \"train\")\n",
    "  val_dir = os.path.join(data_dir, \"val\")\n",
    "\n",
    "  # create train and validation data generators\n",
    "  train_datagen = ImageDataGenerator(\n",
    "      rescale=1. / 255,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True)\n",
    "  val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "  train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(cfg.img_width, cfg.img_height),\n",
    "    batch_size=cfg.batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "  val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(cfg.img_width, cfg.img_height),\n",
    "    batch_size=cfg.batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "\n",
    "  # instantiate model and callbacks\n",
    "  model = finetune_inception_model(cfg.fc_size, cfg.num_classes)\n",
    "  callbacks = [WandbCallback(), ValLog(val_generator, cfg.num_log_batches)]\n",
    "\n",
    "  # train!\n",
    "  model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = cfg.num_train // cfg.batch_size,\n",
    "    epochs=cfg.epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks = callbacks,\n",
    "    validation_steps = cfg.num_val // cfg.batch_size)\n",
    "\n",
    "  # save trained model as artifact\n",
    "  trained_model_artifact = wandb.Artifact(\n",
    "            MODEL_NAME, type=\"model\",\n",
    "            description=\"finetuned inception v3\",\n",
    "            metadata=dict(cfg))\n",
    "  \n",
    "  model.save(SAVE_MODEL_DIR)\n",
    "  trained_model_artifact.add_dir(SAVE_MODEL_DIR)\n",
    "  run.log_artifact(trained_model_artifact)\n",
    "  run.finish()\n",
    "\n",
    "class ValLog(Callback):\n",
    "  \"\"\" Custom callback to log validation images\n",
    "  at the end of each training epoch\"\"\"\n",
    "  def __init__(self, generator=None, num_log_batches=1):\n",
    "    self.generator = generator\n",
    "    self.num_batches = num_log_batches\n",
    "    # store full names of classes\n",
    "    self.flat_class_names = [k for k, v in generator.class_indices.items()]\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    # collect validation data and ground truth labels from generator\n",
    "    val_data, val_labels = zip(*(self.generator[i] for i in range(self.num_batches)))\n",
    "    val_data, val_labels = np.vstack(val_data), np.vstack(val_labels)\n",
    "\n",
    "    # use the trained model to generate predictions for the given number\n",
    "    # of validation data batches (num_batches)\n",
    "    val_preds = self.model.predict(val_data)\n",
    "    true_ids = val_labels.argmax(axis=1)\n",
    "    max_preds = val_preds.argmax(axis=1)\n",
    "\n",
    "    # log validation predictions alongside the run\n",
    "    columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
    "    for a in self.flat_class_names:\n",
    "      columns.append(\"score_\" + a)\n",
    "    predictions_table = wandb.Table(columns = columns)\n",
    "    \n",
    "    # log image, predicted and actual labels, and all scores\n",
    "    for filepath, img, top_guess, scores, truth in zip(self.generator.filenames,\n",
    "                                                       val_data, \n",
    "                                                       max_preds, \n",
    "                                                       val_preds,\n",
    "                                                       true_ids):\n",
    "      img_id = filepath.split('/')[-1].split(\".\")[0]\n",
    "      row = [img_id, wandb.Image(img), \n",
    "             self.flat_class_names[top_guess], self.flat_class_names[truth]]\n",
    "      for s in scores.tolist():\n",
    "        row.append(np.round(s, 4))\n",
    "      predictions_table.add_data(*row)\n",
    "    wandb.run.log({VAL_TABLE_NAME : predictions_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:13jfboot) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worthy-shape-3</strong> at: <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification/runs/13jfboot\" target=\"_blank\">https://wandb.ai/abhilash001vj/Website%20Classification/runs/13jfboot</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230521_180902-13jfboot\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:13jfboot). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\kaggle2022\\web_page_classifier\\notebooks\\wandb\\run-20230521_181345-35t2ts2s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/abhilash001vj/Website%20Classification/runs/35t2ts2s\" target=\"_blank\">base_model</a></strong> to <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification\" target=\"_blank\">https://wandb.ai/abhilash001vj/Website%20Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/abhilash001vj/Website%20Classification/runs/35t2ts2s\" target=\"_blank\">https://wandb.ai/abhilash001vj/Website%20Classification/runs/35t2ts2s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CommError",
     "evalue": "Project abhilash001vj/Website Classification does not contain artifact: \"website_screeshots_80-10-10_3000:latest\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\apis\\normalize.py:26\u001b[0m, in \u001b[0;36mnormalize_exceptions.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\apis\\public.py:949\u001b[0m, in \u001b[0;36mApi.artifact\u001b[1;34m(self, name, type)\u001b[0m\n\u001b[0;32m    948\u001b[0m entity, project, artifact_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_artifact_path(name)\n\u001b[1;32m--> 949\u001b[0m artifact \u001b[39m=\u001b[39m Artifact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient, entity, project, artifact_name)\n\u001b[0;32m    950\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m artifact\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39mtype\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\apis\\public.py:4343\u001b[0m, in \u001b[0;36mArtifact.__init__\u001b[1;34m(self, client, entity, project, name, attrs)\u001b[0m\n\u001b[0;32m   4342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4343\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load()\n\u001b[0;32m   4344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\apis\\public.py:5072\u001b[0m, in \u001b[0;36mArtifact._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5067\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5068\u001b[0m     response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   5069\u001b[0m     \u001b[39mor\u001b[39;00m response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   5070\u001b[0m     \u001b[39mor\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39martifact\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   5071\u001b[0m ):\n\u001b[1;32m-> 5072\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   5073\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mProject \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m does not contain artifact: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   5074\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentity, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_artifact_name)\n\u001b[0;32m   5075\u001b[0m     )\n\u001b[0;32m   5076\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39martifact\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Project abhilash001vj/Website Classification does not contain artifact: \"website_screeshots_80-10-10_3000:latest\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train()\n",
      "Cell \u001b[1;32mIn[55], line 97\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m# locate and download training and validation data\u001b[39;00m\n\u001b[0;32m     96\u001b[0m data_at \u001b[39m=\u001b[39m TRAIN_DATA_AT \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:latest\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 97\u001b[0m data \u001b[39m=\u001b[39m run\u001b[39m.\u001b[39;49muse_artifact(data_at, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbalanced_data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     98\u001b[0m data_dir \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdownload()\n\u001b[0;32m     99\u001b[0m train_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:333\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    332\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_is_attaching \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 333\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2617\u001b[0m, in \u001b[0;36mRun.use_artifact\u001b[1;34m(self, artifact_or_name, type, aliases, use_as)\u001b[0m\n\u001b[0;32m   2615\u001b[0m     name \u001b[39m=\u001b[39m artifact_or_name\n\u001b[0;32m   2616\u001b[0m public_api \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_public_api()\n\u001b[1;32m-> 2617\u001b[0m artifact \u001b[39m=\u001b[39m public_api\u001b[39m.\u001b[39;49martifact(\u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mtype\u001b[39;49m, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   2618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39m!=\u001b[39m artifact\u001b[39m.\u001b[39mtype:\n\u001b[0;32m   2619\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2620\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSupplied type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m does not match type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m of artifact \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2621\u001b[0m             \u001b[39mtype\u001b[39m, artifact\u001b[39m.\u001b[39mtype, artifact\u001b[39m.\u001b[39mname\n\u001b[0;32m   2622\u001b[0m         )\n\u001b[0;32m   2623\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\apis\\normalize.py:64\u001b[0m, in \u001b[0;36mnormalize_exceptions.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[39mraise\u001b[39;00m CommError(message, err)\u001b[39m.\u001b[39mwith_traceback(sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\apis\\normalize.py:26\u001b[0m, in \u001b[0;36mnormalize_exceptions.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhoa, you found a bug.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     28\u001b[0m     \u001b[39mraise\u001b[39;00m CommError(err\u001b[39m.\u001b[39mresponse, err)\n",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\apis\\public.py:949\u001b[0m, in \u001b[0;36mApi.artifact\u001b[1;34m(self, name, type)\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou must specify name= to fetch an artifact.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    948\u001b[0m entity, project, artifact_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_artifact_path(name)\n\u001b[1;32m--> 949\u001b[0m artifact \u001b[39m=\u001b[39m Artifact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient, entity, project, artifact_name)\n\u001b[0;32m    950\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m artifact\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39mtype\u001b[39m:\n\u001b[0;32m    951\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtype \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m specified but this artifact is of type \u001b[39m\u001b[39m{\u001b[39;00martifact\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\apis\\public.py:4343\u001b[0m, in \u001b[0;36mArtifact.__init__\u001b[1;34m(self, client, entity, project, name, attrs)\u001b[0m\n\u001b[0;32m   4341\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs \u001b[39m=\u001b[39m attrs\n\u001b[0;32m   4342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4343\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load()\n\u001b[0;32m   4344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4345\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_description \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\wandb\\apis\\public.py:5072\u001b[0m, in \u001b[0;36mArtifact._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5064\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   5065\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mAttempted to fetch artifact without alias (e.g. \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<artifact_name>:v3\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<artifact_name>:latest\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   5066\u001b[0m         )\n\u001b[0;32m   5067\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5068\u001b[0m     response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   5069\u001b[0m     \u001b[39mor\u001b[39;00m response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   5070\u001b[0m     \u001b[39mor\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39martifact\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   5071\u001b[0m ):\n\u001b[1;32m-> 5072\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   5073\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mProject \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m does not contain artifact: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   5074\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentity, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_artifact_name)\n\u001b[0;32m   5075\u001b[0m     )\n\u001b[0;32m   5076\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39martifact\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   5077\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs\n",
      "\u001b[1;31mCommError\u001b[0m: Project abhilash001vj/Website Classification does not contain artifact: \"website_screeshots_80-10-10_3000:latest\""
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Load model for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional globals to modify\n",
    "# set to a custom name to help keep your experiments organized\n",
    "RUN_NAME = \"\" \n",
    "# change this if you'd like start a new set of comparable Tables\n",
    "# (only Tables logged to the same key can be compared)\n",
    "TEST_TABLE_NAME = \"test_results\" \n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"iv3_finetuned\"\n",
    "# location of test data from our original split\n",
    "# should match SPLIT_DATA_AT\n",
    "TEST_DATA_AT = \"_\".join([PREFIX, \"80-10-10\", str(TOTAL_IMAGES)])\n",
    "\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"inference\", name=RUN_NAME)\n",
    "model_at = run.use_artifact(MODEL_NAME + \":latest\")\n",
    "model_dir = model_at.download()\n",
    "print(\"model: \", model_dir)\n",
    "model = keras.models.load_model(model_dir)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# download latest version of test data\n",
    "test_data_at = run.use_artifact(TEST_DATA_AT + \":latest\")\n",
    "test_dir = test_data_at.download()\n",
    "test_dir += \"/test/\"\n",
    "\n",
    "class_names = [\"Animalia\", \"Amphibia\", \"Arachnida\", \"Aves\", \"Fungi\", \n",
    "               \"Insecta\", \"Mammalia\", \"Mollusca\", \"Plantae\", \"Reptilia\"]\n",
    "\n",
    "# load test images\n",
    "imgs = []\n",
    "filenames = []\n",
    "class_labels = os.listdir(test_dir)\n",
    "truth = []\n",
    "for l in class_labels:\n",
    "  if l.startswith(\".\"):\n",
    "    continue\n",
    "  imgs_per_class = os.listdir(os.path.join(test_dir, l))\n",
    "  for img in imgs_per_class:\n",
    "    # track the image id\n",
    "    filenames.append(img.split(\".\")[0])\n",
    "    truth.append(l)\n",
    "    img_path = os.path.join(test_dir, l, img)\n",
    "    img = image.load_img(img_path, target_size=(299, 299))\n",
    "    img = image.img_to_array(img)\n",
    "    # don't forget to rescale test images to match the range of inputs\n",
    "    # to the network\n",
    "    img = np.expand_dims(img/255.0, axis=0)\n",
    "    imgs.append(img)\n",
    "\n",
    "# predict on test data and bin predictions by guessed label \n",
    "preds = {}\n",
    "imgs = np.vstack(imgs)\n",
    "classes = model.predict(imgs, batch_size=32)\n",
    "for c in classes:\n",
    "  class_id = np.argmax(c)\n",
    "  if class_id in preds:\n",
    "    preds[class_id] += 1\n",
    "  else:\n",
    "    preds[class_id] = 1\n",
    "\n",
    "# log inference results as a Table to the run workspace\n",
    "columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
    "for a in class_names:\n",
    "  columns.append(\"score_\" + a)\n",
    "test_dt = wandb.Table(columns = columns)\n",
    "\n",
    "# store all the scores for each image\n",
    "for img_id, i, t, c in zip(filenames, imgs, truth, classes):\n",
    "  guess = class_names[np.argmax(c)]\n",
    "  row = [img_id, wandb.Image(i), guess, t]\n",
    "  for c_i in c.tolist():\n",
    "    row.append(np.round(c_i, 4))\n",
    "  test_dt.add_data(*row)\n",
    "  \n",
    "run.log({TEST_TABLE_NAME : test_dt})\n",
    "print(\"Quick distribution of predicted classes: \")\n",
    "print(preds)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhil\\miniconda3\\envs\\sd\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "model.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model_ft = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "data_dir = \"../data/website_images_1000/\"\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_dataset = torchvision.datasets.ImageFolder( root=data_dir, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "\ttrain_dataset,\n",
    "\tbatch_size=64,\n",
    "\tshuffle=True,\n",
    "\tnum_workers=4\n",
    "      )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forum', 'general', 'marketplace']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forum': 0, 'general': 1, 'marketplace': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.9253\n",
      "Epoch: 1 Loss: 0.6486\n",
      "Epoch: 2 Loss: 0.5883\n",
      "Epoch: 3 Loss: 0.3971\n",
      "Epoch: 4 Loss: 0.9180\n",
      "Epoch: 5 Loss: 0.5661\n",
      "Epoch: 6 Loss: 0.2214\n",
      "Epoch: 7 Loss: 0.5264\n",
      "Epoch: 8 Loss: 0.2841\n",
      "Epoch: 9 Loss: 0.3814\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "for epoch in range(10):\n",
    "\tfor images, labels in train_loader:\n",
    "\t\timages, labels = images.to(device), labels.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutput = model(images)\n",
    "\t\tloss = criterion(output, labels)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\n",
    "\tprint(\"Epoch: {} Loss: {:.4f}\".format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './website_classifier.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = None\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
