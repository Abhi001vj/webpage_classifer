{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119266"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from web_classifer.data import clean_text\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "raw_data_dir = \"../data/raw_html\"\n",
    "processed_data_dir = \"../data/data.csv\"\n",
    "failed_files_path = \"../data/failed_to_process_files.csv\"\n",
    "\n",
    "labels = [label for label in os.listdir(raw_data_dir)]\n",
    "\n",
    "all_files = glob.glob(f\"{raw_data_dir}/**/*.htm\",recursive=True)\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auto',\n",
       " 'book',\n",
       " 'camera',\n",
       " 'job',\n",
       " 'movie',\n",
       " 'nbaplayer',\n",
       " 'restaurant',\n",
       " 'university']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.79it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "cleaned_text_list = []\n",
    "label_list = []\n",
    "failed_files = []\n",
    "processed_files = []\n",
    "n_rows = 100\n",
    "for data_file in tqdm(all_files[:n_rows]):\n",
    "    try:\n",
    "      html_code=codecs.open(data_file,'r')\n",
    "      soup = BeautifulSoup(html_code, 'html.parser')  #Parse html code\n",
    "      texts = soup.findAll(text=True)                 #find all text\n",
    "      label = re.findall(\"|\".join(labels),data_file)[0]\n",
    "      label_list.append(label)\n",
    "      text_from_html = ' '.join(texts) \n",
    "      data_list.append(text_from_html)\n",
    "      cleaned_text = clean_text(text_from_html)\n",
    "      cleaned_text_list.append(cleaned_text)\n",
    "      processed_files.append(data_file)\n",
    "      \n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        failed_files.append((data_file, str(e)))\n",
    "        \n",
    "failed_df = pd.DataFrame(failed_files, columns=[\"filepath\", \"exception\"]).to_csv(failed_files_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_file_paths = [ file_path for file_path, _ in failed_files]\n",
    "processed_files = [ file_path for file_path in all_files if file_path not in failed_file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119266, 113905, 113905, 113905, 5361, 113905)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files), len(data_list), len(cleaned_text_list), len(label_list), len(failed_files), len(processed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_path</th>\n",
       "      <th>raw_html_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/raw_html\\auto\\auto-autobytel(2000)\\102...</td>\n",
       "      <td>ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...</td>\n",
       "      <td>html public w3c dtd html 4 01 transitional en ...</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/raw_html\\auto\\auto-autobytel(2000)\\102...</td>\n",
       "      <td>ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...</td>\n",
       "      <td>html public w3c dtd html 4 01 transitional en ...</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/raw_html\\auto\\auto-autobytel(2000)\\102...</td>\n",
       "      <td>ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...</td>\n",
       "      <td>html public w3c dtd html 4 01 transitional en ...</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/raw_html\\auto\\auto-autobytel(2000)\\102...</td>\n",
       "      <td>ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...</td>\n",
       "      <td>html public w3c dtd html 4 01 transitional en ...</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/raw_html\\auto\\auto-autobytel(2000)\\102...</td>\n",
       "      <td>ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...</td>\n",
       "      <td>html public w3c dtd html 4 01 transitional en ...</td>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           data_path  \\\n",
       "0  ../data/raw_html\\auto\\auto-autobytel(2000)\\102...   \n",
       "1  ../data/raw_html\\auto\\auto-autobytel(2000)\\102...   \n",
       "2  ../data/raw_html\\auto\\auto-autobytel(2000)\\102...   \n",
       "3  ../data/raw_html\\auto\\auto-autobytel(2000)\\102...   \n",
       "4  ../data/raw_html\\auto\\auto-autobytel(2000)\\102...   \n",
       "\n",
       "                                       raw_html_text  \\\n",
       "0  ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...   \n",
       "1  ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...   \n",
       "2  ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...   \n",
       "3  ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...   \n",
       "4  ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...   \n",
       "\n",
       "                                        cleaned_text label  \n",
       "0  html public w3c dtd html 4 01 transitional en ...  auto  \n",
       "1  html public w3c dtd html 4 01 transitional en ...  auto  \n",
       "2  html public w3c dtd html 4 01 transitional en ...  auto  \n",
       "3  html public w3c dtd html 4 01 transitional en ...  auto  \n",
       "4  html public w3c dtd html 4 01 transitional en ...  auto  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame.from_dict({'data_path': processed_files, 'raw_html_text':data_list, 'cleaned_text':cleaned_text_list, 'label': label_list})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall(\"|\".join(labels),data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"data_path\",\"cleaned_text\",\t\"label\"]\n",
    "data[\"cleaned_text\"] = data[\"cleaned_text\"].apply(lambda x: str(x).encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\"))\n",
    "data.to_csv(\"../data/all_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.groupby('label').sample(n=2000, random_state=42)\n",
    "train_data[cols].to_csv(\"../data/train.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,  test_df = train_test_split(\n",
    "     train_data, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocabulary = 50000\n",
    "text_clf = Pipeline([\n",
    "     ('tfidf',  TfidfVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_features=n_vocabulary)),\n",
    "     ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9884469696969697"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "text_clf.fit(train_df.cleaned_text, train_df.label)\n",
    "\n",
    "predicted = text_clf.predict(test_df.cleaned_text)\n",
    "np.mean(predicted == test_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       movie       1.00      1.00      1.00       646\n",
      "      camera       1.00      0.91      0.95       669\n",
      "         job       0.92      1.00      0.96       695\n",
      "        auto       1.00      1.00      1.00       644\n",
      "   nbaplayer       1.00      1.00      1.00       706\n",
      "  university       1.00      1.00      1.00       645\n",
      "        book       1.00      1.00      1.00       651\n",
      "  restaurant       1.00      1.00      1.00       624\n",
      "\n",
      "    accuracy                           0.99      5280\n",
      "   macro avg       0.99      0.99      0.99      5280\n",
      "weighted avg       0.99      0.99      0.99      5280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test_df.label, predicted,\n",
    "     target_names=test_df.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# cat_cols = []\n",
    "# date_col = []\n",
    "# num_cols = []\n",
    "# text_features = ['num_words', 'num_unique_words', 'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_upper','num_words_title', 'mean_word_len']\n",
    "target = 'label'\n",
    "text_col = 'cleaned_text'\n",
    "# features = cat_cols + num_cols + text_col + text_features + date_features\n",
    "\n",
    "# train_df[cat_cols] = train_df[cat_cols].astype(str)\n",
    "# test_df[cat_cols] = test_df[cat_cols].astype(str)\n",
    "# https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments#Max_df\n",
    "# https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf\n",
    "n_vocabulary = 50000\n",
    "preprocessor = ColumnTransformer(\n",
    "     transformers=[\n",
    "     #     ('numerical', MinMaxScaler(), num_cols),\n",
    "        ('text', TfidfVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_features=n_vocabulary), text_col), #max_df=1.0, min_df=1,\n",
    "     #    ('category', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "     ],\n",
    "remainder='passthrough')\n",
    "\n",
    "train_x = preprocessor.fit_transform(train_df[features])\n",
    "test_x = preprocessor.transform(test_df[features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=10, random_state=42)\n",
    "\n",
    "regr.fit(train_x, train_df[target],n_jobs=-1)\n",
    "\n",
    "predicted = regr.predict(test_x)\n",
    "test_df['score'] = predicted\n",
    "\n",
    "test_df[['id','score']].to_csv('./sk_randomforest_base_line_submission.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 fold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split\n",
    "import lightgbm as lgb\n",
    "# out-of-fold predictions on train data\n",
    "oof = np.zeros(train_x.shape[0])\n",
    "\n",
    "# averaged predictions on train data\n",
    "prediction = np.zeros(test_x.shape[0])\n",
    "\n",
    "# list of scores on folds\n",
    "scores = []\n",
    "feature_importance = pd.DataFrame()\n",
    "# n_estimators = \n",
    "params = {'num_leaves': 128,\n",
    "          'min_child_samples': 100,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 7,\n",
    "          'learning_rate': 0.25,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0\n",
    "         }\n",
    "verbose=500\n",
    "early_stopping_rounds=200\n",
    "n_estimators=3000\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "# split and train on folds\n",
    "# https://www.kaggle.com/artgor/using-meta-features-to-improve-model#Training-separate-models-for-each-type\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(train_x)):\n",
    "\n",
    "    print(f'Training on Fold {fold_n + 1}')\n",
    "    X_train, X_valid = train_x[train_index,:], train_x[valid_index,:]\n",
    "    # y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "    # X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "    y_train, y_valid = train_df[target].iloc[train_index], train_df[target].iloc[valid_index]\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "    model.fit(X_train, y_train, \n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n",
    "            verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "    \n",
    "    y_pred_valid = model.predict(X_valid)\n",
    "    y_pred = model.predict(test_x, num_iteration=model.best_iteration_)\n",
    "        \n",
    "    oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "    scores.append(metrics.mean_absolute_error(y_valid, y_pred_valid))\n",
    "    prediction += y_pred \n",
    "\n",
    "prediction /= folds.n_splits\n",
    "\n",
    "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['score'] = prediction\n",
    "\n",
    "test_df[['id','score']].to_csv(f'./lgbm_{n_fold}_50k_vocabulary_fold_cv_ensemble_submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(train_x, train_df[target])\n",
    "print(\"Grid search CV best parameters\", grid_search.best_params_)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "predicted = best_grid.predict(test_x)\n",
    "test_df['score'] = predicted\n",
    "\n",
    "test_df[['id','score']].to_csv('./sk_randomforest_grid_searchcv_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5c4582e929228def4e8d48d6b59e7b20c465ee4845d4311a80c98754a94d698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
