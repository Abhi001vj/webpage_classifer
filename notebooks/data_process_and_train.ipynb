{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119266"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "sys.path.append(\"../\")\n",
    "from web_classifer.data import clean_text\n",
    "\n",
    "\n",
    "raw_data_dir = \"../data/raw_html\"\n",
    "processed_data_dir = \"../data/data.csv\"\n",
    "failed_files_path = \"../data/failed_to_process_files.csv\"\n",
    "\n",
    "labels = [label for label in os.listdir(raw_data_dir)]\n",
    "\n",
    "all_files = glob.glob(f\"{raw_data_dir}/**/*.htm\",recursive=True)\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auto',\n",
       " 'book',\n",
       " 'camera',\n",
       " 'job',\n",
       " 'movie',\n",
       " 'nbaplayer',\n",
       " 'restaurant',\n",
       " 'university']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = []\n",
    "# cleaned_text_list = []\n",
    "# label_list = []\n",
    "# failed_files = []\n",
    "# processed_files = []\n",
    "# n_rows = len(all_files)\n",
    "# for data_file in tqdm(all_files[:n_rows]):\n",
    "#     try:\n",
    "#       html_code=codecs.open(data_file,'r')\n",
    "#       soup = BeautifulSoup(html_code, 'html.parser')  #Parse html code\n",
    "#       texts = soup.findAll(text=True)                 #find all text\n",
    "#       label = re.findall(\"|\".join(labels),data_file)[0]\n",
    "#       label_list.append(label)\n",
    "#       text_from_html = ' '.join(texts) \n",
    "#       data_list.append(text_from_html)\n",
    "#       cleaned_text = clean_text(text_from_html)\n",
    "#       cleaned_text_list.append(cleaned_text)\n",
    "#       processed_files.append(data_file)\n",
    "      \n",
    "#     except Exception as e:\n",
    "#         # print(e)\n",
    "#         failed_files.append((data_file, str(e)))\n",
    "        \n",
    "# failed_df = pd.DataFrame(failed_files, columns=[\"filepath\", \"exception\"]).to_csv(failed_files_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_file_paths = [ file_path for file_path, _ in failed_files]\n",
    "# processed_files = [ file_path for file_path in all_files if file_path not in failed_file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_files), len(data_list), len(cleaned_text_list), len(label_list), len(failed_files), len(processed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.DataFrame.from_dict({'data_path': processed_files, 'raw_html_text':data_list, 'cleaned_text':cleaned_text_list, 'label': label_list})\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall(\"|\".join(labels),data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"data_path\",\"cleaned_text\",\t\"label\"]\n",
    "# data[\"cleaned_text\"] = data[\"cleaned_text\"].apply(lambda x: str(x).encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\"))\n",
    "# data.to_csv(\"../data/all_data.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned data and split it for tarining and testing based on the website and label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_path</th>\n",
       "      <th>raw_html_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/raw_html\\auto\\auto-autobytel(2000)\\102...</td>\n",
       "      <td>ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...</td>\n",
       "      <td>html public w3c dtd html 4 01 transitional en ...</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto-autobytel(2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/raw_html\\auto\\auto-autobytel(2000)\\102...</td>\n",
       "      <td>ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...</td>\n",
       "      <td>html public w3c dtd html 4 01 transitional en ...</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto-autobytel(2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/raw_html\\auto\\auto-autobytel(2000)\\102...</td>\n",
       "      <td>ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...</td>\n",
       "      <td>html public w3c dtd html 4 01 transitional en ...</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto-autobytel(2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/raw_html\\auto\\auto-autobytel(2000)\\102...</td>\n",
       "      <td>ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...</td>\n",
       "      <td>html public w3c dtd html 4 01 transitional en ...</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto-autobytel(2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/raw_html\\auto\\auto-autobytel(2000)\\102...</td>\n",
       "      <td>ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...</td>\n",
       "      <td>html public w3c dtd html 4 01 transitional en ...</td>\n",
       "      <td>auto</td>\n",
       "      <td>auto-autobytel(2000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           data_path  \\\n",
       "0  ../data/raw_html\\auto\\auto-autobytel(2000)\\102...   \n",
       "1  ../data/raw_html\\auto\\auto-autobytel(2000)\\102...   \n",
       "2  ../data/raw_html\\auto\\auto-autobytel(2000)\\102...   \n",
       "3  ../data/raw_html\\auto\\auto-autobytel(2000)\\102...   \n",
       "4  ../data/raw_html\\auto\\auto-autobytel(2000)\\102...   \n",
       "\n",
       "                                       raw_html_text  \\\n",
       "0  ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...   \n",
       "1  ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...   \n",
       "2  ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...   \n",
       "3  ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...   \n",
       "4  ï»¿ \\n HTML PUBLIC \"-//W3C//DTD HTML 4.01 Tran...   \n",
       "\n",
       "                                        cleaned_text label  \\\n",
       "0  html public w3c dtd html 4 01 transitional en ...  auto   \n",
       "1  html public w3c dtd html 4 01 transitional en ...  auto   \n",
       "2  html public w3c dtd html 4 01 transitional en ...  auto   \n",
       "3  html public w3c dtd html 4 01 transitional en ...  auto   \n",
       "4  html public w3c dtd html 4 01 transitional en ...  auto   \n",
       "\n",
       "                website  \n",
       "0  auto-autobytel(2000)  \n",
       "1  auto-autobytel(2000)  \n",
       "2  auto-autobytel(2000)  \n",
       "3  auto-autobytel(2000)  \n",
       "4  auto-autobytel(2000)  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"website\"] = data.data_path.apply(lambda x: x.split(\"\\\\\")[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"website\"].unique()\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gs = GroupShuffleSplit(n_splits=1, test_size=.6, random_state=42)\n",
    "train_ix, test_ix = next(gs.split(data, groups=data[\"website\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14182, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.iloc[train_ix,:].website.unique()\n",
    "data.iloc[train_ix,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27347, 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.iloc[test_ix,:].website.unique()\n",
    "data.iloc[test_ix,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"data_path\",\"cleaned_text\",\t\"website\", \"label\"]\n",
    "# train_data = data.groupby('label').sample(n=2000, random_state=42)\n",
    "train_data = data.iloc[train_ix,:]\n",
    "train_data[cols].to_csv(\"../data/train.csv\", index=False)\n",
    "test_data = data.iloc[test_ix,:]\n",
    "test_data[cols].to_csv(\"../data/test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and evaluation\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base line model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df,  test_df = train_test_split(\n",
    "     train_data, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocabulary = 50000\n",
    "text_clf = Pipeline([\n",
    "     ('tfidf',  TfidfVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_features=n_vocabulary)),\n",
    "     ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(max_features=50000, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(max_features=50000, stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=50000, stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_features=50000, stop_words='english')),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "text_clf.fit(train_data.cleaned_text, train_data.label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7579624821735473"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf.predict(test_data.cleaned_text)\n",
    "np.mean(predicted == test_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        auto       1.00      0.71      0.83      7631\n",
      "        book       0.99      0.79      0.88     15985\n",
      "      camera       0.22      1.00      0.36      1801\n",
      "         job       1.00      0.46      0.63      1930\n",
      "\n",
      "    accuracy                           0.76     27347\n",
      "   macro avg       0.80      0.74      0.67     27347\n",
      "weighted avg       0.94      0.76      0.81     27347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test_data.label, predicted,\n",
    "     target_names=test_data.label.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# cat_cols = []\n",
    "# date_col = []\n",
    "# num_cols = []\n",
    "# text_features = ['num_words', 'num_unique_words', 'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_upper','num_words_title', 'mean_word_len']\n",
    "target = 'label'\n",
    "text_col = 'cleaned_text'\n",
    "# features = cat_cols + num_cols + text_col + text_features + date_features\n",
    "\n",
    "# train_df[cat_cols] = train_df[cat_cols].astype(str)\n",
    "# test_df[cat_cols] = test_df[cat_cols].astype(str)\n",
    "# https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments#Max_df\n",
    "# https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf\n",
    "n_vocabulary = 50000\n",
    "preprocessor = ColumnTransformer(\n",
    "     transformers=[\n",
    "     #     ('numerical', MinMaxScaler(), num_cols),\n",
    "        ('text', TfidfVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_features=n_vocabulary), text_col), #max_df=1.0, min_df=1,\n",
    "     #    ('category', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "     ],\n",
    "remainder='passthrough')\n",
    "\n",
    "train_x = preprocessor.fit_transform(train_df[features])\n",
    "test_x = preprocessor.transform(test_df[features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=10, random_state=42)\n",
    "\n",
    "regr.fit(train_x, train_df[target],n_jobs=-1)\n",
    "\n",
    "predicted = regr.predict(test_x)\n",
    "test_df['score'] = predicted\n",
    "\n",
    "test_df[['id','score']].to_csv('./sk_randomforest_base_line_submission.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 fold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split\n",
    "import lightgbm as lgb\n",
    "# out-of-fold predictions on train data\n",
    "oof = np.zeros(train_x.shape[0])\n",
    "\n",
    "# averaged predictions on train data\n",
    "prediction = np.zeros(test_x.shape[0])\n",
    "\n",
    "# list of scores on folds\n",
    "scores = []\n",
    "feature_importance = pd.DataFrame()\n",
    "# n_estimators = \n",
    "params = {'num_leaves': 128,\n",
    "          'min_child_samples': 100,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 7,\n",
    "          'learning_rate': 0.25,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0\n",
    "         }\n",
    "verbose=500\n",
    "early_stopping_rounds=200\n",
    "n_estimators=3000\n",
    "n_fold = 5\n",
    "folds = GroupKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "# split and train on folds\n",
    "# https://www.kaggle.com/artgor/using-meta-features-to-improve-model#Training-separate-models-for-each-type\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(train_x)):\n",
    "\n",
    "    print(f'Training on Fold {fold_n + 1}')\n",
    "    X_train, X_valid = train_x[train_index,:], train_x[valid_index,:]\n",
    "    # y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "    # X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "    y_train, y_valid = train_df[target].iloc[train_index], train_df[target].iloc[valid_index]\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "    model.fit(X_train, y_train, \n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n",
    "            verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "    \n",
    "    y_pred_valid = model.predict(X_valid)\n",
    "    y_pred = model.predict(test_x, num_iteration=model.best_iteration_)\n",
    "        \n",
    "    oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "    scores.append(metrics.mean_absolute_error(y_valid, y_pred_valid))\n",
    "    prediction += y_pred \n",
    "\n",
    "prediction /= folds.n_splits\n",
    "\n",
    "print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['score'] = prediction\n",
    "\n",
    "test_df[['id','score']].to_csv(f'./lgbm_{n_fold}_50k_vocabulary_fold_cv_ensemble_submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(train_x, train_df[target])\n",
    "print(\"Grid search CV best parameters\", grid_search.best_params_)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "predicted = best_grid.predict(test_x)\n",
    "test_df['score'] = predicted\n",
    "\n",
    "test_df[['id','score']].to_csv('./sk_randomforest_grid_searchcv_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5c4582e929228def4e8d48d6b59e7b20c465ee4845d4311a80c98754a94d698"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
